{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "The objective of segmentation is to associate each pixel of an image with a well-defined class by providing a map of labels.\n",
    "In this notebook we will use U-Net to perform semantic segmentation.\n",
    "To this end we will use functions that belong to the libraries **keras_hub** and **easy_cv_dataset** that will be installed with the following instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade keras-hub git+https://github.com/davin11/easy-cv-dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the following file `unet_hub.py` must be downloaded with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SITE=\"https://raw.githubusercontent.com/davin11/easy-cv-dataset/master\"\n",
    "!wget -nc {SITE}/examples/segmentation/unet_hub.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will import the stantard libraries, keras_hub, and easy_cv_dataset (with the alias ds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import keras\n",
    "import keras_hub\n",
    "import easy_cv_dataset as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1746373133097,
     "user": {
      "displayName": "davide cozzolino",
      "userId": "01048783342148768927"
     },
     "user_tz": -120
    },
    "id": "_WoO3Zt6SdQh",
    "outputId": "cb31225d-e043-4388-f1a8-944967b9587c"
   },
   "source": [
    "### Data preparation\n",
    "We will use the dataset [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/), that includes images of cats and dogs. For each image a segmentation map is available that identifies the object (foreground) from the background.\n",
    "On notebook you can directly execute the following instructions to download the dataset which is already separated in training, validation and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_7wqvlWRtF1"
   },
   "outputs": [],
   "source": [
    "# Oxford Pets Dataset\n",
    "SITE=\"\"\n",
    "!wget -nc {SITE}/guide_TF/oxford_pets_dataset.zip\n",
    "!unzip -q -n oxford_pets_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you will find a folder called `oxford_pets_dataset` which contains images and segmentation maps.\n",
    "In addition, you will find three CSV (Comma-Separated Values) files: `tab_train.csv`, `tab_val.csv` and `tab_test.csv`, for training, validation and test set, respectively.\n",
    "Files with extension CSV (Comma-Separated Values) are textual files that contain a table and adopt the comma character (,) to separate columns. In our case these files contain two columns: *image* and *segmentation_mask* with the filepath of the images and the segmentation maps.\n",
    "We will use the function `ds.image_segmentation_dataset_from_dataframe` to prepare images in the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Background\", \"Foreground\",]\n",
    "num_classes = 2\n",
    "\n",
    "BATCH_SIZE=16\n",
    "IMAGE_SIZE=160\n",
    "\n",
    "from keras.layers import Resizing, RandomColorDegeneration, RandomRotation, Pipeline\n",
    "\n",
    "pre_batching_processing = Resizing(IMAGE_SIZE, IMAGE_SIZE)\n",
    "post_batching_processing = Pipeline(\n",
    "    layers=[\n",
    "        RandomColorDegeneration(0.5),\n",
    "        RandomRotation((-0.06, 0.06)), # from -6% to +6% of 360Â°\n",
    "    ]\n",
    ")\n",
    "\n",
    "print('test-set')\n",
    "test_ds  = ds.image_segmentation_dataset_from_dataframe('oxford_pets_dataset/tab_test.csv', class_mode='categorical', class_names=class_names,\n",
    "                                                        pre_batching_processing=pre_batching_processing, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('trainig-set')\n",
    "train_ds = ds.image_segmentation_dataset_from_dataframe('oxford_pets_dataset/tab_train.csv', class_mode='categorical', class_names=class_names,\n",
    "                                                        pre_batching_processing=pre_batching_processing, shuffle=True , batch_size=BATCH_SIZE, post_batching_processing=post_batching_processing)\n",
    "\n",
    "print('validetion-set')\n",
    "valid_ds = ds.image_segmentation_dataset_from_dataframe('oxford_pets_dataset/tab_val.csv' , class_mode='categorical', class_names=class_names,\n",
    "                                                        pre_batching_processing=pre_batching_processing, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `ds.image_segmentation_dataset_from_dataframe` requires as first parameter the CSV file.\n",
    "The second parameter `class_mode` indicates the format needed to convert segmentation maps which is `'categorical'`.\n",
    "Finally, the third parameter `class_names` is a list of the classes names.\n",
    "The other parameters present in `da.image_segmentation_dataset_from_dataframe` are the same of\n",
    "the function `ds.image_classification_dataset_from_dataframe`, already seen in previous examples.\n",
    "Note that for all the images of the three datasets it is necessary a resizing operation to `224x224` pixels so that all the images have the same dimension. \n",
    "For the training dataset there are already Data Augmentation operations.\n",
    "Let's use the following instruction to visualize some examples of the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "executionInfo": {
     "elapsed": 2380,
     "status": "ok",
     "timestamp": 1746373221746,
     "user": {
      "displayName": "davide cozzolino",
      "userId": "01048783342148768927"
     },
     "user_tz": -120
    },
    "id": "hkjYmPDXl-sN",
    "outputId": "e35de181-97ed-4c85-a293-6701b9c47678"
   },
   "outputs": [],
   "source": [
    "from easy_cv_dataset.visualization import plot_segmentation_mask_gallery\n",
    "for images, segms in test_ds.take(1): # takes the first batch of test-set\n",
    "  plot_segmentation_mask_gallery( # function to display image and box\n",
    "    images, y_true=segms, num_classes=num_classes\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network definition\n",
    "For this example, we will use the U-Net architecture for image segmentation.\n",
    "The architecture is shown in following figure and consists of two parts: on the left the *encoder*, also called *contracting path* and on the right the *decoder*, called *expansive path*.\n",
    "\n",
    "![U-Net](https://raw.githubusercontent.com/davin11/easy-cv-dataset/master/examples/segmentation/u_net.png)\n",
    "\n",
    "The *encoder* compactly extracts high-level information such as context.\n",
    "While the *decoder* restores the spatial dimensions ensuring precise localization.\n",
    "The *encoder* follows the typical architecture of a convolutional neural network composed of convolutional layers and pooling layers. \n",
    "Pooling layers reduce the spatial dimensions of *feature maps* by halving them each time.\n",
    "Downstream of the *encoder*, the *feature maps* will have a spatial resolution equal to one sixteenth of those of the starting image.\n",
    "The *decoder*, in addition to the classic convolutional layers, provides for operations named *up-conv* which double the spatial dimensions each time.\n",
    "The *up-conv* operation is formed by cascading a nearest-neighbor tween and a `2x2` pixel spatial convolution.\n",
    "\n",
    "The U-Net architecture also includes shortcuts between the two parts, called (*skip connection*),\n",
    "and show gray arrows in the illustration in figure. In particular, the feature-maps calculated at the various levels of the *encoder* are supplied in input to the real levels of the *decoder* by concatenating them with those of the previous level. Using the *skip connection* helps to improve the accuracy of the segmentation map.\n",
    "\n",
    "Note that the U-Net architecture is a fully convolutional network, in fact it does not foresee *fully connected* layers. Therefore it can be applied to images of different sizes. The only constraint is that the number of rows and columns of the input image must be multiples of `16`.\n",
    "In the `unet_hub.py` file there is the `UnetBackbone` function to instantiate the U-Net backbone architecture in KerasHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKy3a-fIGcK2"
   },
   "outputs": [],
   "source": [
    "from unet_hub import UnetBackbone, ImageSemanticSegmenter\n",
    "from keras_hub.layers import ImageConverter\n",
    "from keras_hub.models import ImageSegmenterPreprocessor\n",
    "backbone = UnetBackbone(use_batchnorm=True) \n",
    "normalization = ImageConverter(image_size=(IMAGE_SIZE, IMAGE_SIZE), scale=1./255)\n",
    "model = ImageSemanticSegmenter(\n",
    "    preprocessor = ImageSegmenterPreprocessor(normalization),\n",
    "    backbone = backbone,\n",
    "    num_classes = 2,\n",
    "    activation=None)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `num_classes` specifies the number of output classes, while `use_batchnorm=True` enables the use of batch normalization layers compared to the original architecture described in the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 10756,
     "status": "error",
     "timestamp": 1746550697851,
     "user": {
      "displayName": "davide cozzolino",
      "userId": "01048783342148768927"
     },
     "user_tz": -120
    },
    "id": "tu7M1wm6VH22",
    "outputId": "4c3028d7-8353-4d9d-d3e8-33d3c16ea1ca"
   },
   "source": [
    "### Training\n",
    "In this example, we will use Nadam optimization and as loss function the Focal Loss which is a variant of the cross-entropy loss, very muchused in segmentation problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KPfLDCj6AQv"
   },
   "outputs": [],
   "source": [
    "from keras.losses import CategoricalFocalCrossentropy\n",
    "from keras.optimizers import Nadam\n",
    "from keras.metrics import MeanIoU\n",
    "model.compile(\n",
    "  loss=CategoricalFocalCrossentropy(from_logits=True),\n",
    "  optimizer=Nadam(learning_rate=0.001),\n",
    "  metrics=[MeanIoU(num_classes=2, sparse_y_true=False, sparse_y_pred=False),],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not use accuracy as a performance index, but Intersection over Union (IoU).\n",
    "In fact, for segmentation, accuracy is not a reliable index as it is influenced by the size of the region to be segmented with respect to that of the image.\n",
    "The IoU instead considers the percentage of correct pixels with respect to the size of the region to be segmented and of the predicted region. \n",
    "Parameters `sparse_y_true=False` and `sparse_y_pred=False` indicate to the function that will calculate the IoU that the segmentation maps and the network output respectively are in the format categorical.\n",
    "We carry out the training using the `fit` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293985,
     "status": "ok",
     "timestamp": 1746373551835,
     "user": {
      "displayName": "davide cozzolino",
      "userId": "01048783342148768927"
     },
     "user_tz": -120
    },
    "id": "CfF8bKH46SIa",
    "outputId": "1240cf7a-8e08-4930-84e7-136fd88c8d27"
   },
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=5, validation_data=valid_ds, verbose=True)\n",
    "model.save_weights('net.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the `save_weights` method is used to save the network parameters to a file in the HDF5 format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Let's use the following instructions to see the network result on some examples from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "executionInfo": {
     "elapsed": 2927,
     "status": "ok",
     "timestamp": 1746373554771,
     "user": {
      "displayName": "davide cozzolino",
      "userId": "01048783342148768927"
     },
     "user_tz": -120
    },
    "id": "4H_K4bh16Vwf",
    "outputId": "1efd468d-6f6d-427b-93d7-d776b9d3d65d"
   },
   "outputs": [],
   "source": [
    "from easy_cv_dataset.visualization import plot_segmentation_mask_gallery\n",
    "for images, segms in test_ds.take(1):\n",
    "  pred = model.predict(images)\n",
    "  plot_segmentation_mask_gallery(\n",
    "    images, y_true=segms, y_pred=pred, num_classes=num_classes\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use the function `evaluate` to compute the average intersection over union on the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10137,
     "status": "ok",
     "timestamp": 1746373564913,
     "user": {
      "displayName": "davide cozzolino",
      "userId": "01048783342148768927"
     },
     "user_tz": -120
    },
    "id": "Gz6CpDfB-3aQ",
    "outputId": "0a304ed0-e4c2-4075-c892-694744fc414c"
   },
   "outputs": [],
   "source": [
    "metrics = model.evaluate(test_ds, return_dict=True, verbose=True)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "Now, we will use a partially pre-trained architecture. In particular, instead of the classic encoder of the U-Net architecture, we will use a ResNet50 network, pre-trained for classification on the ImageNet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_hub import UnetBackbone, ImageSemanticSegmenter\n",
    "from keras_hub.models import Backbone, ImageSegmenterPreprocessor\n",
    "from keras_hub.layers import ImageConverter\n",
    "\n",
    "pretrained_model = 'resnet_50_imagenet'\n",
    "image_encoder = Backbone.from_preset(pretrained_model)  # Encoder \n",
    "backbone = UnetBackbone(image_encoder=image_encoder, use_batchnorm=True) # Encoder + Decoder\n",
    "normalization = ImageConverter.from_preset(pretrained_model, image_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "model = ImageSemanticSegmenter(\n",
    "    preprocessor = ImageSegmenterPreprocessor(normalization),\n",
    "    backbone = backbone,\n",
    "    num_classes = 2,\n",
    "    activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the risk of overfitting, we can skip training the first layers of the network. If you don't want to train the entire encoder, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.image_encoder.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf2.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
