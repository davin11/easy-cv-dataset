{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection\n",
    "Aim of this notebook is to perform object detection by means of [RetinaNet](https://arxiv.org/abs/1708.02002v2). \n",
    "Object detection refers to techniques used to identify objects in images or videos. \n",
    "It can be seen as an extension of classification, but instead of classifying the entire image, it detects and localizes an arbitrary number of classes within the image.\n",
    "The output of object detection techniques consists of bounding boxes that enclose the objects of interest present in the image. For each bounding box, a corresponding class label is provided, as shown in the example depicted in the figure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Object Detection](https://raw.githubusercontent.com/davin11/easy-cv-dataset/master/examples/object_detection/objdet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the functions of the KerasNub and easy-cv-dataset libraries, to be installed using the following instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11698,
     "status": "ok",
     "timestamp": 1746549177731,
     "user": {
      "displayName": "davide cozzolino",
      "userId": "01048783342148768927"
     },
     "user_tz": -120
    },
    "id": "C3QwKEEXErGF",
    "outputId": "dbc82e34-5240-4d5f-ce94-880e3aea487d"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade keras-hub git+https://github.com/davin11/easy-cv-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will import the stantard libraries, keras_hub, and easy_cv_dataset (with the alias ds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import keras\n",
    "import keras_hub\n",
    "import easy_cv_dataset as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "We will use the dataset [PascalVOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/), where we have 20 different classes:\n",
    "- **Person**: person\n",
    "- **Animals**: bird, cat, cow, dog, horse, sheep\n",
    "- **Vehicles**: aeroplane, bicycle, boat, bus, car, motorbike, train\n",
    "- **Objects**: bottle, chair, dining table, potted plant, sofa, tvmonitor\n",
    "\n",
    "Write the following instructions to download training, validation and test set for PascalVOC 2007:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ywc6L4FPFMou",
    "outputId": "09594e85-a6ca-41a4-9f67-e2769c53530e"
   },
   "outputs": [],
   "source": [
    "SITE=\"https://raw.githubusercontent.com/davin11/easy-cv-dataset/master\"\n",
    "!wget -nc {SITE}/examples/object_detection/voc2007_objdet_test.csv\n",
    "!wget -nc {SITE}/examples/object_detection/voc2007_objdet_train.csv\n",
    "!wget -nc {SITE}/examples/object_detection/voc2007_objdet_val.csv\n",
    "!wget -nc {SITE}/examples/object_detection/voc2007_download.sh\n",
    "!bash voc2007_download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you will find two folders named `voc2007_trainval` and `voc2007_test` that contain the images of the dataset.\n",
    "Additionally, you will find three CSV (Comma-Separated Values) files: `voc2007_object_train.csv`, `voc2007_object_val.csv`, and `voc2007_object_test.csv`, corresponding to the training, validation and test set, respectively.\n",
    "CSV files are simple text files that contain a table using a comma (,) as the column delimiter.\n",
    "These CSV files for Object Detection contain the position and class of the bounding boxes, and they have 6 columns:\n",
    " `image` with the file paths of the images,\n",
    " `xmin` with the horizontal coordinate of the top-left pixel of each box,\n",
    " `ymin` with the vertical coordinate of the top-left pixel of each box,\n",
    " `xmax` with the horizontal coordinate of the bottom-right pixel of each box,\n",
    " `ymax` with the vertical coordinate of the bottom-right pixel of each box, and\n",
    " `class` containing the class information for each box.\n",
    "Here is an excerpt from the `voc2007_objdet_test.csv` file:\n",
    "\n",
    "```\n",
    "image,xmin,ymin,xmax,ymax,class\n",
    "./voc2007_test/VOCdevkit/VOC2007/JPEGImages/002118.jpg,1,288,189,334,car\n",
    "./voc2007_test/VOCdevkit/VOC2007/JPEGImages/002118.jpg,215,109,487,211,car\n",
    "./voc2007_test/VOCdevkit/VOC2007/JPEGImages/002118.jpg,24,88,102,113,car\n",
    "./voc2007_test/VOCdevkit/VOC2007/JPEGImages/009083.jpg,4,160,497,318,aeroplane\n",
    "./voc2007_test/VOCdevkit/VOC2007/JPEGImages/005800.jpg,1,108,249,348,bus\n",
    "./voc2007_test/VOCdevkit/VOC2007/JPEGImages/005800.jpg,23,3,465,375,person\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an image has multiple bounding boxes, each box should be indicated on a separate row, as shown in the previous example.\n",
    "Now, using the function `ds.image_objdetect_dataset_from_dataframe`, \n",
    "we can specify how to prepare the images for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21805,
     "status": "ok",
     "timestamp": 1746549210067,
     "user": {
      "displayName": "davide cozzolino",
      "userId": "01048783342148768927"
     },
     "user_tz": -120
    },
    "id": "6fw2b6vpGFCU"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=4\n",
    "IMAGE_SIZE=640\n",
    "BOX_FORMAT=\"yxyx\"\n",
    "\n",
    "from keras.layers import Resizing, RandomBrightness, RandomFlip, Pipeline\n",
    "pre_processing = Resizing(IMAGE_SIZE, IMAGE_SIZE,\n",
    "                          pad_to_aspect_ratio=True,\n",
    "\t\t\t\t\t\t  bounding_box_format=BOX_FORMAT)\n",
    "\n",
    "augmenter = Pipeline(layers=[\n",
    "\t\tRandomBrightness(factor=(-0.1, 0.1), value_range=(0, 255), bounding_box_format=BOX_FORMAT),\n",
    "\t\tRandomFlip(\"horizontal\", bounding_box_format=BOX_FORMAT),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4E3jH0A3G2At"
   },
   "outputs": [],
   "source": [
    "print(\"test-set\")\n",
    "test_ds = ds.image_objdetect_dataset_from_dataframe(\"voc2007_objdet_test.csv\",\n",
    "      bounding_box_format=BOX_FORMAT,\n",
    "      pre_batching_processing=pre_processing,\n",
    "      shuffle=False, batch_size=BATCH_SIZE)\n",
    "print(\"trainig-set\")\n",
    "train_ds = ds.image_objdetect_dataset_from_dataframe(\"voc2007_objdet_train.csv\",\n",
    "      bounding_box_format=BOX_FORMAT,\n",
    "      pre_batching_processing=pre_processing,\n",
    "      shuffle=True, batch_size=BATCH_SIZE,\n",
    "      post_batching_processing=augmenter)\n",
    "print(\"validetion-set\")\n",
    "valid_ds = ds.image_objdetect_dataset_from_dataframe(\"voc2007_objdet_val.csv\",\n",
    "      bounding_box_format=BOX_FORMAT,\n",
    "      pre_batching_processing=pre_processing,\n",
    "      shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `image_objdetect_dataset_from_dataframe` requires the CSV file as the first parameter.\n",
    "The second parameter `bounding_box_format` indicates the format in which to format the bounding boxes.\n",
    "Note that the `bounding_box_format` parameter needs to be specified consistently across all components that process the bounding boxes.\n",
    "You can find information about the bounding box formats in the official Keras documentation.\n",
    "The other parameters of the `ds.image_objdetect_dataset_from_dataframe` function are the same of the `ds.image_classification_dataset_from_dataframe` function, which we have seen in other examples.\n",
    "\n",
    "Please note that all images in the three datasets have been resized to `640 x 640` pixels because the PascalVOC images have different dimensions.\n",
    "Additionally, for using the RetinaNet architecture, the images need to have dimensions divisible by 64.\n",
    "Data shuffling and data augmentation operations are only applied to the training set.\n",
    "To visualize some examples from the test set, you can use the following instructions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "output_embedded_package_id": "1rjLxdplQjLJf2AELpL1qLkKJMGCF0tgG"
    },
    "id": "aFdWaVqQILNl",
    "outputId": "f4c005d7-726e-4bb2-ae3e-e69b19d3f86c"
   },
   "outputs": [],
   "source": [
    "from easy_cv_dataset.visualization import plot_bounding_box_gallery\n",
    "for images, boxes in test_ds.take(1): # takes the first batch of test-set\n",
    "    plot_bounding_box_gallery( # function to display image and box\n",
    "        images, y_true=boxes,\n",
    "        bounding_box_format=BOX_FORMAT,\n",
    "        scale=5, font_scale=0.7,\n",
    "        class_mapping=test_ds.class_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network definition\n",
    "We use the functions of KerasHib to create a RetinaNet network with a ResNet50 encoder, which is pre-trained on COCO dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_hub.models import RetinaNetBackbone, RetinaNetObjectDetector, RetinaNetObjectDetectorPreprocessor\n",
    "from keras_hub.layers import RetinaNetImageConverter\n",
    "\n",
    "pretrained_model = \"retinanet_resnet50_fpn_coco\"\n",
    "backbone = RetinaNetBackbone.from_preset(pretrained_model)\n",
    "normalization = RetinaNetImageConverter.from_preset(pretrained_model, image_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "preprocessor = RetinaNetObjectDetectorPreprocessor(normalization)\n",
    "model = RetinaNetObjectDetector(\n",
    "    preprocessor=preprocessor,\n",
    "    backbone=backbone,\n",
    "    num_classes=20,\n",
    "    bounding_box_format=BOX_FORMAT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `num_classes` indicates the number of classes which is set to 20 and is consistent with the dataset we used.\n",
    "To reduce the risk of overfitting, we can choose not to train the first layers of the network. To exclude the entire encoder from training, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "In this example, we use an optimizer called Nadam with gradient clipping.\n",
    "This is done to address the common problem of gradient explosion during the training of object detection networks.\n",
    "For the learning rate, we will use a scheduler that reduces it by a factor of 10 after the first 10 epochs and by an additional factor of 10 after another 20 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpmtQu_fJNMi"
   },
   "outputs": [],
   "source": [
    "base_lr = 0.001\n",
    "from keras import optimizers\n",
    "lr_decay = optimizers.schedules.PiecewiseConstantDecay( # schedulatore del lr\n",
    "  boundaries=[10*len(train_ds), 20*len(train_ds)],\n",
    "  values=[base_lr, 0.1 * base_lr, 0.01 * base_lr],\n",
    ")\n",
    "optimizer = optimizers.Nadam(\n",
    "  learning_rate=lr_decay, global_clipnorm=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For object detection problems, we use the defaout loss functions: the SmoothL1 distance for box localization and the Focal loss for box classification.\n",
    "Focal loss is a variant of the cross-entropy loss that assigns a higher weight to difficult-to-classify elements.\n",
    "To define these loss functions, we use the method `compile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyVF8vgSJi8H"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  box_loss=\"auto\",\n",
    "  classification_loss=\"auto\",\n",
    "  optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40928,
     "status": "ok",
     "timestamp": 1746548358078,
     "user": {
      "displayName": "davide cozzolino",
      "userId": "01048783342148768927"
     },
     "user_tz": -120
    },
    "id": "GZbvbMuMax47",
    "outputId": "f6edb6d3-07a9-4bc7-d84f-12068178b916"
   },
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=30, validation_data = train_ds, verbose=True)\n",
    "model.save_weights('net.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to save the weights after training.\n",
    "You can also load a pre-trained network on PascalVOC 2007 using the following instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc \"https://huggingface.co/datasets/davin11/VOC2007/resolve/main/retinanet_resnet50_fpn_pascalvoc.weights.h5\"\n",
    "model.load_weights('retinanet_resnet50_fpn_pascalvoc.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following instructions to see the result of the network on some examples from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3287,
     "status": "ok",
     "timestamp": 1746548361366,
     "user": {
      "displayName": "davide cozzolino",
      "userId": "01048783342148768927"
     },
     "user_tz": -120
    },
    "id": "1cCmU8gt3LdK",
    "outputId": "ead05f90-8699-422e-8e40-45963d897ff5"
   },
   "outputs": [],
   "source": [
    "from easy_cv_dataset.visualization import plot_bounding_box_gallery\n",
    "for images, boxes in test_ds.take(4):\n",
    "  boxes_pred = model.predict(images)\n",
    "  plot_bounding_box_gallery(\n",
    "      images, y_pred=boxes_pred, #y_true=boxes,\n",
    "      scale=5, font_scale=0.7,\n",
    "      bounding_box_format=BOX_FORMAT,\n",
    "      class_mapping= test_ds.class_names,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The widely used metric in object detection is mean Average Precision (mAP), but evaluating it during training can be computationally expensive. We can use the function `compute_mAP_metrics` to evaluate the mean Average-Precision (mAP) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 699183,
     "status": "ok",
     "timestamp": 1746549081234,
     "user": {
      "displayName": "davide cozzolino",
      "userId": "01048783342148768927"
     },
     "user_tz": -120
    },
    "id": "y2ogAlYkbCys",
    "outputId": "71fb032d-702c-485b-8865-3502d8a147a2"
   },
   "outputs": [],
   "source": [
    "from easy_cv_dataset.metrics import compute_mAP_metrics\n",
    "print(compute_mAP_metrics(model, test_ds, BOX_FORMAT))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOjINfdIjCOBkWgZbKi3Y2N",
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "tf2.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
